{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f0e9e4cd097eea26b9ee573887ebfb8",
     "grade": false,
     "grade_id": "cell-142eb68406c17bde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Week 11 Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af46d7acaf03c9f8f0cc75367d7eca4f",
     "grade": false,
     "grade_id": "cell-b4f848964553845a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 1: Implementing an objective function in NumPy and PyTorch\n",
    "Implement two Python functions that implements the mathematical functions $f(x) = 3x^\\intercal x - x_1 - 4$. One function should work for $x$ a `numpy.ndarray` and the other should work for $x$ a `torch.FloatTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e0b6b47664be091903b36d15dd2dd55",
     "grade": false,
     "grade_id": "cell-182a74ac56520133",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def f_numpy(x):\n",
    "    return 3 * np.dot(x, x) - x[0] - 4\n",
    "\n",
    "\n",
    "def f_torch(x):\n",
    "    return 3 * torch.dot(x, x) - x[0] - 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79d3784e218d88f020e401dba8892966",
     "grade": true,
     "grade_id": "cell-e1c4981cbc9d429f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(f_numpy(np.zeros(4)) + 4) < 1e-6\n",
    "assert abs(f_numpy(np.ones(2)) - 1) < 1e-6\n",
    "\n",
    "assert abs(f_torch(torch.zeros(4)) + 4) < 1e-6\n",
    "assert abs(f_torch(torch.ones(2)) - 1) < 1e-6\n",
    "\n",
    "x0 = (1, -2, 3, 4, 5)\n",
    "assert abs(f_numpy(np.array(x0)) - f_torch(torch.Tensor(x0))) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd0de5c4ef42e55ac7143b61c51034df",
     "grade": false,
     "grade_id": "cell-336788388c9fb151",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 2: Computing gradients manually and automatically\n",
    "Implement functions to compute the gradient of $f$ at $x$ using numpy and PyTorch. Use autodiff for the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aba0ede5b1590bb67e8e425ac1c81202",
     "grade": false,
     "grade_id": "cell-fb660c996d6b60a5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def f_grad_numpy(x):\n",
    "    gradient = 6 * x\n",
    "    gradient[0] -= 1\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def f_grad_torch(x):\n",
    "    f_x = 3 * torch.dot(x, x) - x[0] - 4\n",
    "    gradient = torch.autograd.grad(f_x, x)[0]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "213c40607b7d7239eb81dacac235423b",
     "grade": true,
     "grade_id": "cell-27382f3ccab6772b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def finite_diff(x, v):\n",
    "    return (f_numpy(x + v) - f_numpy(x - v)) / (2 * np.linalg.norm(v))\n",
    "\n",
    "\n",
    "x = np.ones(2)\n",
    "v0 = 1e-4 * np.array([1, 0])\n",
    "assert abs(finite_diff(x, v0) - f_grad_numpy(x)[0]) < 1e-2\n",
    "v1 = 1e-4 * np.array([0, 1])\n",
    "assert abs(finite_diff(x, v1) - f_grad_numpy(x)[1]) < 1e-2\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(10):\n",
    "    x2 = np.random.randn(5)\n",
    "    v2 = np.random.randn(5)\n",
    "    v2 = v2 / np.linalg.norm(v2)\n",
    "    observed = finite_diff(x2, 1e-4 * v2)\n",
    "    derived_vec = f_grad_numpy(x2)\n",
    "    derived = derived_vec.dot(v2)\n",
    "    assert abs(observed - derived) < 1e-2\n",
    "\n",
    "xt = torch.tensor(x, requires_grad=True)\n",
    "diff = f_grad_torch(xt).numpy() - f_grad_numpy(x)\n",
    "assert np.linalg.norm(diff) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
